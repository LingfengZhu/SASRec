# general
gpu_id: 0
use_gpu: True
seed: 2020
state: INFO
reproducibility: True
data_path: 'dataset/'
checkpoint_dir: 'saved'
show_progress: True
save_dataset: False
save_dataloaders: False

# training settings
epochs: 300
train_batch_size: 2048
learner: adam
learning_rate: 0.001
neg_sampling:
  uniform: 1
eval_step: 1
stopping_step: 10
clip_grad_norm: ~
# clip_grad_norm:  {'max_norm': 5, 'norm_type': 2}
weight_decay: 0.0
require_pow: False
load_col: # 指定加载哪些 column；参见数据集的 .inter 文件
    inter: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list_field', 'item_length']

# evaluation settings
# #{'LS':[0.8,0.1,0.1]}

# evaluation 时的参数；在同一数据集上对比不同模型时要注意保证此处的 eval 参数一致
eval_args:
  split: {'LS': 'valid_and_test'}
  group_by: user
  order: TO
  mode: full
repeatable: True
metrics: ["Recall","MRR","NDCG","Hit","Precision"]
topk: [10]
valid_metric: MRR@10
valid_metric_bigger: True
eval_batch_size: 4096
loss_decimal_place: 4
metric_decimal_place: 4


# 模型超参数
## SASRec 部分
n_layers: 2 # Transformer block 的个数
n_heads: 2 # multi-head 的头数
hidden_size: 64 # Transformer 的输入/输出维度
inner_size: 256 # Transformer 内部的 ffn 层（feed-forward net）即 mlp 层（multilater perceptron）的维度，一般为 hidden_size 的 4 倍
hidden_dropout_prob: 0.1 # hidden states 的 dropout 率
attn_dropout_prob: 0.1 # self-attention 的 dropout 率
hidden_act: 'gelu' # 激活函数
layer_norm_eps: 1e-12 # layer normalization 参数，一般不用管
initializer_range: 0.02 # 参数初始化设置：在 -0.02 到 0.02 范围内从高斯分布初始化模型的所有参数
loss_type: 'CE' # 损失函数：Cross Entropy 交叉熵
## 对比学习部分
lmd: 0.1 # 对比学习中损失的权重
lmd_sem: 0.1
aug: 'cts' # 两种对比学习方式：'self' 对应 self-CTS，'CTS' 对应 ST-CTS
